# Docker environment variables configuration
# Copy this file to .env and customize as needed

# ==============================
# Backend Configuration
# ==============================
BACKEND_PORT=8000
BACKEND_WORKERS=1
PYTHONUNBUFFERED=1

# ==============================
# Frontend Configuration
# ==============================
FRONTEND_PORT=80

# ==============================
# Model Configuration
# ==============================
# Path to your llama.cpp model file
MODEL_PATH=/app/api/models/mistral/mistral-7b-instruct-v0.2.Q4_K_M.gguf

# GPU acceleration (0 = CPU only, >0 = number of layers to offload to GPU)
GPU_LAYERS=0

# Model inference parameters
MAX_TOKENS=512
CONTEXT_LENGTH=2048
THREADS=8

# ==============================
# Vector Store Configuration
# ==============================
# Choose: 'faiss' or 'qdrant'
VECTOR_STORE=faiss

# Qdrant settings (if using Qdrant)
QDRANT_HOST=qdrant
QDRANT_PORT=6333

# FAISS index path
FAISS_INDEX_PATH=/app/data/embeddings

# ==============================
# Embedding Model
# ==============================
EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2

# ==============================
# Session Management
# ==============================
SESSION_TIMEOUT_HOURS=24
CLEANUP_INTERVAL_SECONDS=3600

# ==============================
# Redis Configuration (Optional)
# ==============================
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0

# ==============================
# CORS Settings
# ==============================
# Comma-separated list of allowed origins
# Use '*' for development, specific domains for production
ALLOWED_ORIGINS=*

# ==============================
# Logging
# ==============================
LOG_LEVEL=INFO
